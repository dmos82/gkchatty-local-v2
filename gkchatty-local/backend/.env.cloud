# ==========================================
# GKCHATTY CLOUD MODE CONFIGURATION
# ==========================================
# This template configures GKChatty to use cloud services:
# - MongoDB (local or Atlas)
# - Pinecone (cloud vector database)
# - OpenAI API (LLM + embeddings)
#
# Copy this file to .env and fill in your credentials
# ==========================================

# ==========================================
# CORE SETTINGS
# ==========================================
PORT=4001
NODE_ENV=development
JWT_SECRET=change_this_to_a_strong_random_secret_in_production

# Encryption key for sensitive data (API keys stored in DB)
# Generate with: openssl rand -hex 32
ENCRYPTION_KEY=change_this_to_64_character_hex_string_generated_with_openssl_rand_hex_32

# ==========================================
# STORAGE MODE
# ==========================================
GKCHATTY_STORAGE=cloud

# ==========================================
# DATABASE - MongoDB
# ==========================================
# Option 1: Local MongoDB
MONGODB_URI=mongodb://localhost:27017/gkchatty

# Option 2: MongoDB Atlas (recommended for production)
# MONGODB_URI=mongodb+srv://<username>:<password>@<cluster>.mongodb.net/gkchatty?retryWrites=true&w=majority

# ==========================================
# VECTOR DATABASE - Pinecone
# ==========================================
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=gkchatty-dev

# Get these from: https://app.pinecone.io
# Create index with:
# - Dimensions: 1536 (for OpenAI text-embedding-3-small)
# - Metric: cosine
# - Pod Type: p1.x1 (starter) or higher

# ==========================================
# LLM PROVIDER - OpenAI
# ==========================================
OPENAI_API_KEY=your-openai-api-key

# Chat models
OPENAI_PRIMARY_CHAT_MODEL=gpt-4o-mini
OPENAI_FALLBACK_CHAT_MODEL=gpt-3.5-turbo

# Embedding model
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Get API key from: https://platform.openai.com/api-keys
# Ensure billing is enabled

# ==========================================
# FILE STORAGE (Optional)
# ==========================================
# Storage mode for uploaded files
# Options: S3 (AWS S3) | local (local filesystem)
FILE_STORAGE_MODE=S3

# Local file storage directory (used when FILE_STORAGE_MODE=local)
LOCAL_FILE_STORAGE_DIR=uploads

# AWS S3 for file storage (used when FILE_STORAGE_MODE=S3)
# Uncomment and configure for S3 storage
# AWS_REGION=us-east-2
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key
# AWS_BUCKET_NAME=gkchatty-uploads

# ==========================================
# LOGGING
# ==========================================
LOG_LEVEL=debug
# Options: debug | info | warn | error

# ==========================================
# FRONTEND CONFIGURATION
# ==========================================
# Frontend URL(s) for CORS (comma-separated for multiple origins)
FRONTEND_URL=http://localhost:4003

# Production example with multiple origins:
# FRONTEND_URL=https://your-app.com,https://www.your-app.com,https://staging.your-app.com

# ==========================================
# SECURITY (Production)
# ==========================================
# CORS_ORIGIN=https://your-frontend-domain.com
# SESSION_TIMEOUT=3600
# MAX_SESSIONS_PER_USER=10
# ENABLE_RATE_LIMITING=true

# ==========================================
# CACHE & SESSIONS (Optional)
# ==========================================
# Redis URL for session storage and caching
# Leave commented to use in-memory sessions (not recommended for production)
# REDIS_URL=redis://localhost:6379

# For Redis Cloud or production:
# REDIS_URL=redis://:password@your-redis-host:6379

# ==========================================
# FEATURE FLAGS (Phase 4)
# ==========================================
# Control rollout of new features
# All default to false for safety

# Enable Ollama local model integration
FEATURE_OLLAMA_MODELS=false

# Enable smart model routing based on query complexity
FEATURE_SMART_ROUTING=false

# Show which model answered each message
FEATURE_SHOW_MODEL_USED=false

# Ollama Configuration (used when FEATURE_OLLAMA_MODELS=true)
# Only needed if you want to use local models alongside cloud
# OLLAMA_BASE_URL=http://localhost:11434

# ==========================================
# NOTES
# ==========================================
# - Never commit this file with real credentials
# - Use different secrets for dev/staging/production
# - Rotate JWT_SECRET regularly
# - Enable billing for OpenAI before using
# - Use MongoDB Atlas for production (not local MongoDB)
# - Consider using environment-specific indexes in Pinecone
