# ==========================================
# GKCHATTY LOCAL MODE CONFIGURATION
# ==========================================
# This template configures GKChatty to use local services:
# - SQLite (local database)
# - ChromaDB (local vector database)
# - Ollama (local LLM)
#
# ⚠️ FUTURE FEATURE - NOT YET FULLY IMPLEMENTED
# Local mode is planned but not yet integrated
# For now, use .env.cloud for cloud mode
#
# Copy this file to .env when local mode is ready
# ==========================================

# ==========================================
# CORE SETTINGS
# ==========================================
PORT=4001
NODE_ENV=development
JWT_SECRET=change_this_to_a_strong_random_secret_in_production

# Encryption key for sensitive data (API keys stored in DB)
# Generate with: openssl rand -hex 32
ENCRYPTION_KEY=change_this_to_64_character_hex_string_generated_with_openssl_rand_hex_32

# ==========================================
# STORAGE MODE
# ==========================================
GKCHATTY_STORAGE=local

# ==========================================
# LOCAL STORAGE PATHS
# ==========================================
GKCHATTY_HOME=~/.gkchatty
# This directory will contain:
# - data/gkchatty.db (SQLite database)
# - data/vectors/ (ChromaDB vectors)
# - data/models/ (Downloaded embedding models)
# - uploads/ (Uploaded files)

# ==========================================
# DATABASE - SQLite
# ==========================================
# SQLite database file location
SQLITE_DB_PATH=~/.gkchatty/data/gkchatty.db

# SQLite configuration
SQLITE_JOURNAL_MODE=WAL
SQLITE_SYNCHRONOUS=NORMAL
SQLITE_CACHE_SIZE=2000

# ==========================================
# VECTOR DATABASE - ChromaDB
# ==========================================
# ChromaDB local storage path
CHROMA_PERSIST_DIR=~/.gkchatty/data/vectors

# ChromaDB collection name
CHROMA_COLLECTION_NAME=gkchatty_documents

# ==========================================
# LLM PROVIDER - Ollama (Local)
# ==========================================
OLLAMA_BASE_URL=http://localhost:11434

# Chat models (must be pulled via `ollama pull <model>`)
OLLAMA_PRIMARY_CHAT_MODEL=llama3.2:3b
OLLAMA_FALLBACK_CHAT_MODEL=qwen2.5:3b

# Alternative models (for different use cases):
# - llama3.2:1b (fastest, simple queries)
# - llama3.2:3b (balanced, recommended)
# - qwen2.5:7b (slower, complex queries)
# - llama3.1:8b (largest, best quality)

# ==========================================
# EMBEDDINGS - Local (Transformers.js)
# ==========================================
# Embedding model (Hugging Face)
EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2

# Alternative models:
# - Xenova/all-MiniLM-L6-v2 (384 dim, 80MB, fast)
# - Xenova/all-mpnet-base-v2 (768 dim, 420MB, better quality)
# - Xenova/paraphrase-multilingual-MiniLM-L12-v2 (384 dim, multilingual)

# Model cache directory
TRANSFORMERS_CACHE=~/.gkchatty/data/models

# ==========================================
# FILE STORAGE - Local Filesystem
# ==========================================
UPLOAD_DIR=~/.gkchatty/uploads
MAX_FILE_SIZE=10485760  # 10MB in bytes

# ==========================================
# LOGGING
# ==========================================
LOG_LEVEL=debug
# Options: debug | info | warn | error

# ==========================================
# FRONTEND CONFIGURATION
# ==========================================
# Frontend URL(s) for CORS (comma-separated for multiple origins)
FRONTEND_URL=http://localhost:4003

# ==========================================
# PERFORMANCE TUNING
# ==========================================
# Ollama configuration
OLLAMA_NUM_THREADS=4
OLLAMA_NUM_GPU=0  # Set to 1 if you have GPU support

# Embedding batch size
EMBEDDING_BATCH_SIZE=32

# Vector search configuration
VECTOR_SEARCH_TOP_K=5
VECTOR_SEARCH_THRESHOLD=0.7

# ==========================================
# FEATURE FLAGS (Phase 4)
# ==========================================
# Control rollout of new features
# In local mode, Ollama is the primary model provider

# Enable Ollama local model integration (recommended for local mode)
FEATURE_OLLAMA_MODELS=true

# Enable smart model routing based on query complexity
FEATURE_SMART_ROUTING=true

# Show which model answered each message
FEATURE_SHOW_MODEL_USED=true

# ==========================================
# SECURITY (Production)
# ==========================================
# SESSION_TIMEOUT=3600
# MAX_SESSIONS_PER_USER=10
# ENABLE_RATE_LIMITING=true

# ==========================================
# MIGRATION (Future)
# ==========================================
# When switching from cloud to local mode
# MIGRATE_FROM_CLOUD=false
# CLOUD_MONGODB_URI=mongodb+srv://...
# CLOUD_PINECONE_API_KEY=...

# ==========================================
# SETUP INSTRUCTIONS
# ==========================================
# Before using local mode:
#
# 1. Install Ollama:
#    curl -fsSL https://ollama.com/install.sh | sh
#
# 2. Pull required models:
#    ollama pull llama3.2:3b
#    ollama pull qwen2.5:3b
#
# 3. Verify Ollama is running:
#    curl http://localhost:11434/api/tags
#
# 4. Create GKChatty directory:
#    mkdir -p ~/.gkchatty/{data,uploads}
#
# 5. Copy this file to .env:
#    cp .env.local .env
#
# 6. Start backend:
#    npm run dev
#
# 7. On first run:
#    - SQLite database auto-created
#    - ChromaDB initialized
#    - Embedding models downloaded (~80MB-420MB)
#
# ==========================================
# NOTES
# ==========================================
# - Local mode = 100% offline capable
# - Zero cloud costs
# - Complete data privacy
# - 10-20x faster than cloud mode (no API latency)
# - Requires ~2-5GB disk space total
# - First run downloads embedding models (one-time, ~5 min)
# - Ollama models downloaded separately via `ollama pull`
#
# Status: PLANNED (not yet fully integrated)
# See CLEANUP-AND-MERGE-PLAN.md for implementation timeline
